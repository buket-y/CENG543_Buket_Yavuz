# RAG vs LLM-Only: PubMedQA Ãœzerinde KarÅŸÄ±laÅŸtÄ±rmalÄ± DeÄŸerlendirme

Bu proje, Retrieval-Augmented Generation (RAG) ve sadece LLM (Large Language Model) yaklaÅŸÄ±mlarÄ±nÄ± PubMedQA veri seti Ã¼zerinde karÅŸÄ±laÅŸtÄ±rmalÄ± olarak deÄŸerlendirmektedir. DPR (Dense Passage Retrieval) ve SBERT (Sentence-BERT) tabanlÄ± RAG sistemleri ile Ã§eÅŸitli LLM modellerinin performansÄ±nÄ± F1 skoru, doÄŸruluk ve gÃ¼venilirlik (faithfulness) metrikleri ile Ã¶lÃ§mektedir. AyrÄ±ca detaylÄ± gÃ¶rselleÅŸtirme ve analiz araÃ§larÄ± iÃ§ermektedir.

## Ä°Ã§indekiler

- [Proje Ã–zeti](#proje-Ã¶zeti)
- [Mimari](#mimari)
- [Kurulum](#kurulum)
- [KullanÄ±m](#kullanÄ±m)
- [SonuÃ§lar](#sonuÃ§lar)
- [Proje YapÄ±sÄ±](#proje-yapÄ±sÄ±)
- [Teknik Detaylar](#teknik-detaylar)
- [Yeni Ã–zellikler](#yeni-Ã¶zellikler)
- [Gelecek Ä°yileÅŸtirmeler](#gelecek-iyileÅŸtirmeler)

## Proje Ã–zeti

Bu proje, biyomedikal soru-cevap gÃ¶revinde RAG sisteminin performansÄ±nÄ± deÄŸerlendirmek iÃ§in tasarlanmÄ±ÅŸtÄ±r. FarklÄ± yaklaÅŸÄ±mlar karÅŸÄ±laÅŸtÄ±rÄ±lmaktadÄ±r:

1. **LLM-Only**: LLM modellerinin sadece kendi bilgisiyle cevap Ã¼retmesi
2. **DPR-RAG**: DPR (Dense Passage Retrieval) ile belge eriÅŸimi yapÄ±p, bu belgeleri kullanarak cevap Ã¼retmesi
3. **SBERT-RAG**: SBERT (Sentence-BERT) ile belge eriÅŸimi yapÄ±p, bu belgeleri kullanarak cevap Ã¼retmesi

### KullanÄ±lan Teknolojiler

- **LLM Modelleri**: 
  - Mistral-7B-Instruct-v0.2
  - Phi-3-mini-4k-instruct
- **Retrieval Modelleri**: 
  - **DPR (Dense Passage Retrieval)**: Facebook DPR
    - Question Encoder: `facebook/dpr-question_encoder-single-nq-base`
    - Context Encoder: `facebook/dpr-ctx_encoder-single-nq-base`
  - **SBERT (Sentence-BERT)**: `sentence-transformers/all-MiniLM-L6-v2`
- **Vector Database**: FAISS (Facebook AI Similarity Search)
- **Veri Seti**: PubMedQA (Ã¶rnek sayÄ±sÄ±/alt-kÃ¼me seÃ§imi deney ayarlarÄ±na gÃ¶re deÄŸiÅŸebilir)
- **GÃ¶rselleÅŸtirme**: Matplotlib, Seaborn

## Mimari

### RAG Pipeline

**DPR-RAG:**
```
Soru â†’ DPR Question Encoder â†’ Query Embedding
                                    â†“
                            FAISS Index (DPR Context Embeddings)
                                    â†“
                            Top-K Document Retrieval
                                    â†“
                    Context + Question â†’ LLM â†’ Cevap
```

**SBERT-RAG:**
```
Soru â†’ SBERT Encoder â†’ Query Embedding
                            â†“
                    FAISS Index (SBERT Embeddings)
                            â†“
                    Top-K Document Retrieval
                            â†“
            Context + Question â†’ LLM â†’ Cevap
```

### LLM-Only Pipeline

```
Soru â†’ Mistral-7B â†’ Cevap
```

## ğŸ”§ Kurulum

### Gereksinimler

- Python 3.8+
- CUDA destekli GPU (Ã¶nerilir, CPU da Ã§alÄ±ÅŸÄ±r)
- ~15GB disk alanÄ± (modeller iÃ§in)

### AdÄ±mlar

1. **Repository'yi klonlayÄ±n:**
```bash
git clone <repository-url>
cd bkt
```

2. **Sanal ortam oluÅŸturun ve aktifleÅŸtirin:**
```bash
python -m venv rag543
# Windows
rag543\Scripts\activate
# Linux/Mac
source rag543/bin/activate
```

3. **BaÄŸÄ±mlÄ±lÄ±klarÄ± yÃ¼kleyin:**
```bash
pip install -r requirements.txt
```

4. **Ek baÄŸÄ±mlÄ±lÄ±klar (eÄŸer eksikse):**
```bash
pip install datasets transformers torch faiss-cpu scikit-learn tqdm sentence-transformers matplotlib seaborn
```

## ğŸ“– KullanÄ±m

### Tek Komutla UÃ§tan Uca Ã‡alÄ±ÅŸtÄ±rma (Opsiyonel)

TÃ¼m adÄ±mlarÄ± (indirme â†’ hazÄ±rlama â†’ index â†’ deney â†’ deÄŸerlendirme â†’ plot) tek seferde Ã§alÄ±ÅŸtÄ±rmak iÃ§in:

```bash
bash run_project_pipeline.sh
```

Notlar:
- `run_project_pipeline.sh` **bash** gerektirir. Windowsâ€™ta **WSL** veya **Git Bash** ile Ã§alÄ±ÅŸtÄ±rÄ±n.
- Ã‡alÄ±ÅŸma logâ€™u kÃ¶k dizinde `project_execution.log` olarak oluÅŸur.

### 1. Veri Setini Ä°ndirme

PubMedQA veri setini indirin:
```bash
python scripts/download_pubmedqa.py
```

### 2. Corpus HazÄ±rlama

PubMedQA context'lerini iÅŸlenebilir TSV formatÄ±na dÃ¶nÃ¼ÅŸtÃ¼rÃ¼n:
```bash
python scripts/prepare_pubmed_corpus.py
```

EÄŸer corpus ID'lerinde sorun varsa:
```bash
python scripts/fix_corpus_ids.py
```

### 3. Embedding ve Index OluÅŸturma

**DPR Index OluÅŸturma:**
DPR Context Encoder ile corpus'u encode edin ve FAISS index'i oluÅŸturun:
```bash
python scripts/index.py
```

Bu iÅŸlem ÅŸunlarÄ± oluÅŸturur:
- `dpr_ctx_embeddings.npy`: DPR context embedding'leri (kÃ¶k dizinde)
- `dpr_faiss.index`: DPR FAISS index dosyasÄ± (kÃ¶k dizinde)

**SBERT Index OluÅŸturma (Opsiyonel):**
SBERT modeli ile corpus'u encode edin ve FAISS index'i oluÅŸturun:
```bash
python scripts/build_sbert_index.py
```

Bu iÅŸlem ÅŸunlarÄ± oluÅŸturur:
- `sbert_faiss.index`: SBERT tabanlÄ± FAISS index dosyasÄ± (kÃ¶k dizinde)

### 4. Deneyleri Ã‡alÄ±ÅŸtÄ±rma

LLM-only ve RAG deneylerini Ã§alÄ±ÅŸtÄ±rÄ±n:
```bash
python scripts/run_experiments.py
```

Bu script ÅŸunlarÄ± oluÅŸturur:
- `llm_only_<LLM>.jsonl`: LLM-only sonuÃ§larÄ± (Ã¶rn. `llm_only_Mistral-7B.jsonl`, `llm_only_Phi-3.jsonl`)
- `rag_<LLM>_<Retriever>.jsonl`: RAG sonuÃ§larÄ± (Ã¶rn. `rag_Mistral-7B_DPR.jsonl`, `rag_Phi-3_SBERT.jsonl`)

### 5. DeÄŸerlendirme

#### F1 Skoru ve DoÄŸruluk DeÄŸerlendirmesi
```bash
python scripts/evaluate_f1.py
```

#### GÃ¼venilirlik (Faithfulness) DeÄŸerlendirmesi
```bash
python scripts/evaluate_faithfulness.py
```

#### GÃ¶rselleÅŸtirme ve Analiz
SonuÃ§larÄ± gÃ¶rselleÅŸtirmek ve detaylÄ± analiz yapmak iÃ§in:
```bash
python scripts/analysis_plots.py
```

Bu script ÅŸu gÃ¶rselleÅŸtirmeleri oluÅŸturur:
- LLM-Only ve RAG iÃ§in confusion matrix'ler
- Soru bazlÄ± doÄŸruluk karÅŸÄ±laÅŸtÄ±rmasÄ±
- Retrieval skor daÄŸÄ±lÄ±mÄ±
- GÃ¼venilirlik vs DoÄŸruluk scatter plot'u
Ve Ã§Ä±ktÄ±larÄ± `results/` klasÃ¶rÃ¼ne kaydeder.

### Tekil Test

Tek bir soru iÃ§in test yapmak isterseniz:

**LLM-Only:**
```bash
python scripts/run_llm_only.py
```

**RAG (DPR):**
```bash
python scripts/run_rag.py
```

**SBERT Retriever Test:**
SBERT retriever'Ä± doÄŸrudan test etmek iÃ§in:
```bash
python scripts/retrieve.py
```

Bu script SBERT retriever'Ä± kullanarak Ã¶rnek bir sorgu Ã§alÄ±ÅŸtÄ±rÄ±r.

## SonuÃ§lar

### F1 Skoru ve DoÄŸruluk Metrikleri

En gÃ¼ncel metrikler `results/metrics_summary.csv` ve Ã¶zet rapor `results/summary_report.txt` iÃ§inde bulunur.

AÅŸaÄŸÄ±daki tablo, mevcut `results/metrics_summary.csv` Ã§Ä±ktÄ±sÄ±ndaki deÄŸerleri Ã¶zetler:

| Model | Samples | Macro-F1 | Accuracy |
|------|---------:|---------:|---------:|
| LLM-only (Mistral-7B) | 1000 | 0.2756 | 0.326 |
| LLM-only (Phi-3) | 1000 | 0.1822 | 0.181 |
| RAG (Mistral-7B + DPR) | 1000 | 0.2524 | 0.315 |
| RAG (Mistral-7B + SBERT) | 1000 | 0.2698 | 0.308 |
| RAG (Phi-3 + DPR) | 1000 | 0.3151 | 0.367 |
| RAG (Phi-3 + SBERT) | 1000 | 0.3128 | 0.371 |

### GÃ¼venilirlik (Faithfulness) Metrikleri

Faithfulness metrikleri sadece RAG koÅŸullarÄ± iÃ§in hesaplanÄ±r. En gÃ¼ncel Ã¶zet `results/summary_report.txt` ve `results/faithfulness_summary.csv` dosyalarÄ±ndadÄ±r.

### SonuÃ§ Analizi

`results/summary_report.txt` Ã¶zetine gÃ¶re:

1. **En iyi Accuracy**: 0.3710 (RAG (Phi-3 + SBERT))
2. **En iyi Macro-F1**: 0.3151 (RAG (Phi-3 + DPR))
3. **Genel gÃ¶zlem**: RAG, LLM-onlyâ€™e gÃ¶re bazÄ± karÅŸÄ±laÅŸtÄ±rmalarda daha iyi sonuÃ§ verir; kesin hÃ¼kÃ¼m iÃ§in aynÄ± LLM ile koÅŸul bazÄ±nda kÄ±yas yapmak gerekir.

### DetaylÄ± Batch SonuÃ§ Analizi

#### 1. SÄ±nÄ±f BazlÄ± Performans Analizi

**LLM-Only Model:**
- Model, Ã§oÄŸu durumda "yes" cevabÄ± verme eÄŸilimindedir
- "maybe" ve "no" sÄ±nÄ±flarÄ±nÄ± ayÄ±rt etmekte zorlanmaktadÄ±r
- Genel olarak daha uzun ve detaylÄ± cevaplar Ã¼retmektedir (ortalama ~500-800 karakter)
- Cevap Ã¼retiminde kendi eÄŸitim bilgisini kullanmaktadÄ±r, bu da bazen yanlÄ±ÅŸ bilgilere yol aÃ§abilmektedir

**DPR-RAG Model:**
- Retrieve edilen belgelere dayalÄ± cevaplar Ã¼retmektedir
- "Insufficient evidence" ifadesini sÄ±kÃ§a kullanmaktadÄ±r (gÃ¼venilirlik aÃ§Ä±sÄ±ndan olumlu)
- Cevap uzunluklarÄ± daha kÄ±sa ve odaklÄ±dÄ±r (ortalama ~200-400 karakter)
- Retrieve edilen belgelerin kalitesi cevap kalitesini doÄŸrudan etkilemektedir

#### 2. Retrieval Kalitesi Analizi

**Retrieval SkorlarÄ±:**
- En yÃ¼ksek similarity skoru: ~0.69 (mitochondria ve PCD sorusu)
- En dÃ¼ÅŸÃ¼k similarity skoru: ~0.59 (bazÄ± genel tÄ±bbi sorular)
- Ortalama similarity skoru: ~0.64
- Skorlar 0.59-0.69 aralÄ±ÄŸÄ±nda daÄŸÄ±lmÄ±ÅŸtÄ±r, bu da retrieval sisteminin tutarlÄ± Ã§alÄ±ÅŸtÄ±ÄŸÄ±nÄ± gÃ¶stermektedir

**Retrieval BaÅŸarÄ±sÄ±:**
- Top-1 belge genellikle soruyla ilgili iÃ§erik saÄŸlamaktadÄ±r
- Ancak bazÄ± durumlarda retrieve edilen belgeler soruyu tam olarak yanÄ±tlamamaktadÄ±r
- Bu durumda model "Insufficient evidence" cevabÄ± vermektedir

#### 3. Cevap Ãœretim Kalitesi KarÅŸÄ±laÅŸtÄ±rmasÄ±

**LLM-Only Ã–zellikleri:**
- Daha akÄ±cÄ± ve doÄŸal dil kullanÄ±mÄ±
- DetaylÄ± aÃ§Ä±klamalar ve baÄŸlam saÄŸlama
- Bazen yanlÄ±ÅŸ veya gÃ¼ncel olmayan bilgiler
- Hallucination riski yÃ¼ksek
- PubMedQA formatÄ±na uyum sorunu (yes/no/maybe)

**DPR-RAG Ã–zellikleri:**
- Retrieve edilen belgelere dayalÄ±, daha gÃ¼venilir cevaplar
- "Insufficient evidence" kullanÄ±mÄ± ile ÅŸeffaflÄ±k
- Daha kÄ±sa ve odaklÄ± cevaplar
- Bazen retrieve edilen belgeler yetersiz kalabilmektedir
- Cevap Ã¼retiminde daha mekanik bir dil kullanÄ±mÄ±

#### 4. Hata Analizi

**LLM-Only HatalarÄ±:**
1. **SÄ±nÄ±f YanlÄ±ÅŸ SÄ±nÄ±flandÄ±rmasÄ±**: "yes" sorusuna "no" veya "maybe" cevabÄ± verme
2. **AÅŸÄ±rÄ± Genelleme**: Model kendi bilgisini kullanarak kesin cevaplar verme eÄŸilimi
3. **Format UyumsuzluÄŸu**: Uzun aÃ§Ä±klamalar yerine kÄ±sa yes/no/maybe bekleniyor

**DPR-RAG HatalarÄ±:**
1. **Retrieval BaÅŸarÄ±sÄ±zlÄ±ÄŸÄ±**: Ä°lgili belgelerin retrieve edilememesi
2. **Yetersiz Context**: Retrieve edilen belgelerin soruyu tam yanÄ±tlayamamasÄ±
3. **AÅŸÄ±rÄ± Ä°htiyatlÄ±lÄ±k**: Yeterli bilgi olsa bile "Insufficient evidence" verme

#### 5. BaÅŸarÄ±lÄ± Ã–rnekler

**RAG BaÅŸarÄ±lÄ± Ã–rnekler:**
- **Soru**: "30-Day and 1-year mortality in emergency general surgery laparotomies..."
- **SonuÃ§**: Retrieve edilen belgeler doÄŸrudan ilgili istatistikleri iÃ§ermekte, model doÄŸru cevap Ã¼retebilmektedir
- **Retrieval Skoru**: 0.69 (yÃ¼ksek)

**LLM-Only BaÅŸarÄ±lÄ± Ã–rnekler:**
- **Soru**: "Do mitochondria play a role in remodelling lace plant leaves..."
- **SonuÃ§**: Model genel bilgisiyle doÄŸru yÃ¶nde cevap verebilmektedir
- **Not**: Ancak bu tÃ¼r baÅŸarÄ±lar tutarsÄ±zdÄ±r

#### 6. Ä°yileÅŸtirme Ã–nerileri

**Retrieval Ä°yileÅŸtirmeleri:**
- Top-K deÄŸerini artÄ±rmak (5 â†’ 10)
- Query expansion teknikleri kullanmak
- Hybrid retrieval (dense + sparse) denemek
- Re-ranking mekanizmasÄ± eklemek

**Generation Ä°yileÅŸtirmeleri:**
- Prompt engineering ile format uyumunu artÄ±rmak
- Few-shot Ã¶rnekler eklemek
- Temperature ve sampling parametrelerini optimize etmek
- Post-processing ile cevap normalizasyonu

**Sistem Ä°yileÅŸtirmeleri:**
- Daha bÃ¼yÃ¼k corpus kullanmak
- Domain-specific fine-tuning
- Ensemble yÃ¶ntemleri
- Active learning ile veri toplama

#### 7. Ä°statistiksel Ã–zet

**Cevap UzunluklarÄ±:**
- **LLM-Only**: Ortalama ~650 karakter, aralÄ±k: 200-1200 karakter
- **DPR-RAG**: Ortalama ~350 karakter, aralÄ±k: 100-800 karakter
- RAG cevaplarÄ± daha kÄ±sa ve odaklÄ±dÄ±r

**"Insufficient Evidence" KullanÄ±mÄ±:**
- RAG modeli, retrieve edilen belgelerin yetersiz olduÄŸu durumlarda ÅŸeffaflÄ±k saÄŸlamak iÃ§in bu ifadeyi kullanmaktadÄ±r
- Bu yaklaÅŸÄ±m, gÃ¼venilirlik aÃ§Ä±sÄ±ndan olumlu bir Ã¶zelliktir
- LLM-only model bu ifadeyi kullanmamaktadÄ±r

**Retrieval DaÄŸÄ±lÄ±mÄ±:**
- Top-1 belge skorlarÄ±: 0.59 - 0.69 aralÄ±ÄŸÄ±nda
- SkorlarÄ±n Ã§oÄŸu 0.62-0.67 aralÄ±ÄŸÄ±nda yoÄŸunlaÅŸmÄ±ÅŸtÄ±r
- Bu, retrieval sisteminin tutarlÄ± Ã§alÄ±ÅŸtÄ±ÄŸÄ±nÄ± gÃ¶stermektedir

**SÄ±nÄ±f DaÄŸÄ±lÄ±mÄ± (Gold Labels):**
- Bu daÄŸÄ±lÄ±m, seÃ§ilen Ã¶rnek sayÄ±sÄ±na ve kullanÄ±lan alt-kÃ¼meye gÃ¶re deÄŸiÅŸir.
- En doÄŸru Ã¶zet iÃ§in deney Ã§Ä±ktÄ±larÄ±nÄ± (`llm_only_*.jsonl`, `rag_*.jsonl`) ve raporu (`results/summary_report.txt`) referans alÄ±n.

#### 8. Performans KarÅŸÄ±laÅŸtÄ±rma Tablosu

Bu bÃ¶lÃ¼mdeki metrikleri statik yazmak yerine, `results/metrics_summary.csv` ve `results/summary_report.txt` dosyalarÄ±nÄ± kaynak olarak kullanÄ±n (deney ayarlarÄ± deÄŸiÅŸtikÃ§e deÄŸerler gÃ¼ncellenir).

#### 9. Ã–rnek Ã‡Ä±ktÄ±lar

**Ã–rnek 1: BaÅŸarÄ±lÄ± RAG CevabÄ±**

**Soru**: "30-Day and 1-year mortality in emergency general surgery laparotomies: an area of concern and need for improvement?"

**Retrieved Documents**: Emergency surgery mortality istatistikleri iÃ§eren belgeler (skor: 0.69)

**RAG CevabÄ±**: 
> "Based on the context, the 30-day mortality was 12.4% for patients under 70 years and 22% for those over 70 years, and the 1-year mortality was 25.1% for patients under 70 years and 38% for those over 70 years. These findings suggest that there is a higher mortality rate for older patients both in the short and long term. Therefore, it appears that there is a need for improvement in the area of emergency general surgery laparotomies, particularly for older patients."

**DeÄŸerlendirme**:  Retrieve edilen belgeler doÄŸrudan ilgili, cevap belgelere dayalÄ±

---

**Ã–rnek 2: LLM-Only vs RAG KarÅŸÄ±laÅŸtÄ±rmasÄ±**

**Soru**: "Do mitochondria play a role in remodelling lace plant leaves during programmed cell death?"

**LLM-Only CevabÄ±**:
> "Mitochondria do play a role in programmed cell death (PCD) in plants, including the remodeling of lace plant leaves. During PCD, mitochondria undergo various changes, such as swelling, membrane potential loss, and the release of reactive oxygen species (ROS) and other pro-apoptotic factors..."

**RAG CevabÄ±**:
> "Based on the context, there is evidence that mitochondria play a role in programmed cell death (PCD) in the lace plant (Aponogeton madagascariensis). The study examines mitochondrial dynamics during PCD and shows that mitochondrial permeability transition pore (PTP) formation may be involved. However, the exact role of mitochondria in remodelling the lace plant leaves during PCD is not explicitly stated in the context. Therefore, the answer is: Insufficient evidence."

**DeÄŸerlendirme**: 
- LLM-Only: Genel bilgi kullanarak kesin cevap veriyor (hallucination riski)
- RAG: Retrieve edilen belgelere dayalÄ±, ÅŸeffaf ve gÃ¼venilir cevap

#### 10. SonuÃ§ ve Ã–neriler

**Ana Bulgular:**
1. Retrieval (DPR/SBERT) seÃ§imi ve kullanÄ±lan LLM, performansÄ± belirgin ÅŸekilde etkiler.
2. RAG iÃ§in faithfulness metrikleri `results/faithfulness_summary.csv` iÃ§inde Ã¶zetlenir.
3. Retrieval kalitesi, RAG performansÄ±nÄ± doÄŸrudan etkiler.

**Pratik Ã–neriler:**
- Biyomedikal QA gÃ¶revleri iÃ§in RAG yaklaÅŸÄ±mÄ± Ã¶nerilir
- Retrieval sisteminin optimize edilmesi kritik Ã¶neme sahiptir
- Prompt engineering ile format uyumu artÄ±rÄ±labilir
- Daha bÃ¼yÃ¼k ve kaliteli corpus kullanÄ±mÄ± performansÄ± artÄ±racaktÄ±r

##  Proje YapÄ±sÄ±

```
bkt/
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ raw/
â”‚   â”‚   â””â”€â”€ pubmedqa.json              # Ham PubMedQA veri seti
â”‚   â””â”€â”€ processed/
â”‚       â”œâ”€â”€ pubmed_corpus.tsv           # Ä°ÅŸlenmiÅŸ corpus
â”‚       â””â”€â”€ pubmed_corpus_fixed.tsv     # ID'leri dÃ¼zeltilmiÅŸ corpus
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ download_pubmedqa.py           # Veri seti indirme
â”‚   â”œâ”€â”€ prepare_pubmed_corpus.py       # Corpus hazÄ±rlama
â”‚   â”œâ”€â”€ fix_corpus_ids.py              # Corpus ID dÃ¼zeltme
â”‚   â”œâ”€â”€ index.py                       # DPR FAISS index oluÅŸturma
â”‚   â”œâ”€â”€ build_sbert_index.py           # SBERT FAISS index oluÅŸturma
â”‚   â”œâ”€â”€ retrieve.py                    # DPR ve SBERT ile belge eriÅŸimi
â”‚   â”œâ”€â”€ generate.py                    # LLM ile cevap Ã¼retme
â”‚   â”œâ”€â”€ run_experiments.py             # Toplu deney Ã§alÄ±ÅŸtÄ±rma
â”‚   â”œâ”€â”€ run_llm_only.py                # LLM-only test
â”‚   â”œâ”€â”€ run_rag.py                     # RAG test
â”‚   â”œâ”€â”€ evaluate_f1.py                 # F1 ve doÄŸruluk deÄŸerlendirme
â”‚   â”œâ”€â”€ evaluate_faithfulness.py       # GÃ¼venilirlik deÄŸerlendirme
â”‚   â””â”€â”€ analysis_plots.py              # GÃ¶rselleÅŸtirme ve analiz
â”œâ”€â”€ results/                           # Raporlar, CSV Ã¶zetler ve gÃ¶rseller
â”‚   â”œâ”€â”€ metrics_summary.csv
â”‚   â”œâ”€â”€ faithfulness_summary.csv
â”‚   â”œâ”€â”€ summary_report.txt
â”‚   â””â”€â”€ *.png
â”œâ”€â”€ dpr_ctx_embeddings.npy             # DPR context embedding'leri (kÃ¶k dizin)
â”œâ”€â”€ dpr_faiss.index                    # DPR FAISS index (kÃ¶k dizin)
â”œâ”€â”€ sbert_faiss.index                  # SBERT FAISS index (kÃ¶k dizin)
â”œâ”€â”€ llm_only_*.jsonl                   # LLM-only sonuÃ§larÄ± (kÃ¶k dizin)
â”œâ”€â”€ rag_*.jsonl                        # RAG sonuÃ§larÄ± (kÃ¶k dizin)
â”œâ”€â”€ requirements.txt                   # Python baÄŸÄ±mlÄ±lÄ±klarÄ±
â”œâ”€â”€ run_project_pipeline.sh            # UÃ§tan uca pipeline (bash)
â””â”€â”€ README.md                          # Bu dosya
```

## ğŸ”¬ Teknik Detaylar

### Model KonfigÃ¼rasyonu

- **LLM Model**: `mistralai/Mistral-7B-Instruct-v0.2`
- **Device**: CUDA (varsa), aksi halde CPU
- **Dtype**: float16 (CUDA), float32 (CPU)
- **Max Tokens**: 128 (varsayÄ±lan)

### Retrieval KonfigÃ¼rasyonu

**DPR:**
- **Top-K**: 5 (retrieve edilen belge sayÄ±sÄ±)
- **Embedding Dimension**: 768 (DPR base model)
- **Similarity Metric**: Cosine Similarity (Inner Product)
- **Max Length**: 512 token

**SBERT:**
- **Top-K**: 5 (retrieve edilen belge sayÄ±sÄ±)
- **Embedding Dimension**: 384 (all-MiniLM-L6-v2)
- **Similarity Metric**: Cosine Similarity (Inner Product)
- **Model**: `sentence-transformers/all-MiniLM-L6-v2`

### Prompt TasarÄ±mÄ±

**RAG Modu:**
```
You are a biomedical question answering assistant.

Answer the question ONLY using the information provided in the context.
Do NOT use any external knowledge.
If the answer cannot be derived from the context, say "Insufficient evidence."

Context:
[Retrieved documents]

Question:
[Question]

Answer:
```

**LLM-Only Modu:**
```
You are a biomedical question answering assistant.

Answer the following question to the best of your knowledge.

Question:
[Question]

Answer:
```

### DeÄŸerlendirme Metrikleri

1. **Macro-F1**: TÃ¼m sÄ±nÄ±flar (yes/no/maybe) iÃ§in ortalama F1 skoru
2. **Accuracy**: DoÄŸru tahmin yÃ¼zdesi
3. **Citation Overlap**: Cevaptaki token'larÄ±n retrieve edilen belgelerdeki token'larla Ã¶rtÃ¼ÅŸme oranÄ±
4. **Attribution Score**: Cevaptaki cÃ¼mlelerin retrieve edilen belgeler tarafÄ±ndan desteklenme oranÄ±

##  Yeni Ã–zellikler

### SBERT (Sentence-BERT) DesteÄŸi
- SBERT tabanlÄ± alternatif retrieval sistemi eklendi
- DPR'ye gÃ¶re daha hÄ±zlÄ± ve hafif bir alternatif
- `build_sbert_index.py` scripti ile SBERT index'i oluÅŸturulabilir
- `SBERTRetriever` sÄ±nÄ±fÄ± ile kullanÄ±labilir

### GÃ¶rselleÅŸtirme ve Analiz
- `analysis_plots.py` scripti ile detaylÄ± gÃ¶rselleÅŸtirmeler
- Confusion matrix'ler (LLM-Only ve RAG)
- Soru bazlÄ± doÄŸruluk karÅŸÄ±laÅŸtÄ±rmasÄ±
- Retrieval skor daÄŸÄ±lÄ±mÄ± analizi
- GÃ¼venilirlik vs doÄŸruluk korelasyon analizi

### Ã‡oklu LLM DesteÄŸi
- Mistral-7B-Instruct
- Phi-3-mini-4k-instruct
- `run_experiments.py` ile farklÄ± LLM'ler test edilebilir

##  Gelecek Ä°yileÅŸtirmeler

- [x] SBERT retrieval desteÄŸi
- [x] GÃ¶rselleÅŸtirme ve analiz araÃ§larÄ±
- [x] Ã‡oklu LLM desteÄŸi
- [ ] Daha bÃ¼yÃ¼k Ã¶rnek seti ile deneyler (50 â†’ 500+)
- [ ] Hybrid retrieval (DPR + SBERT) denemeleri
- [ ] Re-ranking mekanizmasÄ± eklenmesi

##  Notlar

- Ä°lk Ã§alÄ±ÅŸtÄ±rmada modeller otomatik olarak indirilecektir (~15GB)
- GPU kullanÄ±mÄ± Ã¶nerilir, ancak CPU ile de Ã§alÄ±ÅŸÄ±r (daha yavaÅŸ)
- FAISS index oluÅŸturma iÅŸlemi corpus boyutuna gÃ¶re zaman alabilir

### Ã‡Ä±ktÄ±lar Nerede?

- **Veri**: `data/raw/` ve `data/processed/`
- **Index dosyalarÄ±**: kÃ¶k dizinde `dpr_faiss.index`, `sbert_faiss.index` (ve `dpr_ctx_embeddings.npy`)
- **Deney Ã§Ä±ktÄ± dosyalarÄ±**: kÃ¶k dizinde `llm_only_*.jsonl`, `rag_*.jsonl`
- **Rapor/plot**: `results/`

##  Lisans

Bu proje eÄŸitim amaÃ§lÄ± geliÅŸtirilmiÅŸtir.

##  Yazar

Term Project - RAG vs LLM-Only KarÅŸÄ±laÅŸtÄ±rmasÄ±

---

**Son GÃ¼ncelleme**: 2024

